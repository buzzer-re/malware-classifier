import os, sys
import argparse
import json
import pickle
import numpy as np
import tqdm
import matplotlib.pyplot as plt
from math import ceil
from random import shuffle
from sklearn import metrics
from sklearn.metrics import plot_roc_curve
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_extraction import FeatureHasher
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

from feature_extractor import get_features, compile_rules

classifiers = {
    'Random Forest': {
        'instance': RandomForestClassifier(64),
        'save_path': 'random_forest_database.pkl',
        'trained_data': None,
        'results': 0
    },
    'KNN': {
        'instance': KNeighborsClassifier(5),
        'save_path': 'knn_database.pkl',
        'trained_data': None,
        'results': 0
    },
    'Gaussian Naive Bayes': {
        'instance': GaussianNB(),
        'save_path': 'gaussian.pkl',
        'trained_data': None,
        'results': 0
    },
}

MALICIOUS_THRESHOLD = 0.6

def process_features(args):
    features, hasher, pbar = args
    sample_type = features.get('malicious', None)
    if sample_type:
        features.pop('malicious')

    hashed_features = hasher.transform([features])
    hashed_features = hashed_features.toarray()[0]

    return hashed_features, sample_type


def train(dataset, return_X_y = False):
    hasher = FeatureHasher(2000)

    X = []
    y = []
    print(f"Processing {len(dataset)} features...")
    for feature in dataset:
        args = (feature, hasher, None)
        hashed, sample_type = process_features(args)
        X.append(hashed)
        y.append(sample_type)

    if return_X_y:
        return (X, y)

    fit(X, y, hasher=hasher)

def fit(X, y, hasher=None):
    '''
    Fit our classifiers in dictionary
    :X features
    :y classes
    '''
    for classifier_name, classifier_data in classifiers.items():
        classifier = classifier_data.get('instance')
        database_path = classifier_data.get('save_path')

        print(f"Training with {classifier_name}")
        classifier.fit(X,y)
        
        if hasher:
            print(f"Saving at {database_path}...")
            with open(database_path, "wb") as temp_db:
                pickle.dump((classifier, hasher), temp_db)

            print("Saved")


def scan_file(file_path):
    features = get_features(file_path)

    if features is None:
        return None

    features = dict(features)
    for classifier_name,  classifier_data in classifiers.items():
        if classifier_data['trained_data'] is None:
            database_path = classifier_data.get('save_path')
            
            if not os.access(database_path, os.F_OK):
                continue

            with open(database_path, "rb") as saved_detector:
                classifier, hasher = pickle.load(saved_detector)
                classifier_data['trained_data'] = (classifier, hasher)

        else:
            classifier, hasher = classifier_data['trained_data']
        
        hashed_features, s_type = process_features((features, hasher, None))

        if hasattr(classifier, "decision_function"):
            results_prob = classifier.decision_function([hashed_features])
        else:
            results_prob = classifier.predict_proba([hashed_features])[:,-1]

        classifier_data['results'] += 1 if results_prob >= MALICIOUS_THRESHOLD else 0



def evaluate(dataset):
    '''
    Compute classifier accuracty using Cross-Validantion and ROC curve
    '''
    X, y = train(dataset, return_X_y=True)
    X, y = np.array(X), np.array(y)
    
    cv = StratifiedKFold(n_splits=3)
    fig, ax = plt.subplots()    
    for classifier_name, classifier_data in classifiers.items():
        classifier = classifier_data.get('instance')
        for i, (train_data, test) in enumerate(cv.split(X, y)):
            classifier.fit(X[train_data], y[train_data])
            viz = plot_roc_curve(classifier, X[test], y[test],
                         name='Validação cruzada curva ROC {}'.format(classifier_name),
                         ax=ax)
    
    plt.xlabel("Taxa de falso positivos")
    plt.ylabel("Taxa de detecção")
    plt.legend()
    plt.grid()
    plt.title("Validação cruzada AUROC")
    plt.show()
        

def load_dataset(dataset_path):
    if not os.access(dataset_path, os.F_OK):
        raise Exception("Unable to access {}".format(dataset_path))

    dataset = []
    with open(dataset_path, "r") as dataset_fd:
        for line in dataset_fd:
            dict_loaded = json.loads(line)
            dataset.append(dict_loaded)
    
    return dataset


if __name__ == '__main__':
    parser = argparse.ArgumentParser("Scan PE files using a classifier")
    parser.add_argument("--file", help="Path to file to do the scan", nargs='+')
    parser.add_argument("--train", help="Train our models first", action='store_true')
    parser.add_argument("--dataset", help="dataset path if we are training", type=str, default='')
    parser.add_argument("--malware", help="Flag to change the name of the saved statistics file", action='store_true')
    parser.add_argument("--evaluate", help="Evaluate classifier accuaracy", action='store_true')
    args = parser.parse_args()

    if args.train:
        if args.dataset == '':
            print("You should pass the dataset path")
            sys.exit(1)
        
        dataset = load_dataset(args.dataset)
        train(dataset)

    if args.evaluate:
        if args.dataset == '':
            print("You should pass the dataset path")
            sys.exit(1)

        dataset = load_dataset(args.dataset)
        evaluate(dataset)

    elif args.file:
        total_files = len(args.file)
        save_file = 'results_{}.json'.format('malware' if args.malware else 'clean')
        classifier_names = ','.join([classifier_name for classifier_name in classifiers.keys()])
        print(f"Analyzing {total_files} files with {classifier_names}")
        results = 0
        pbar = tqdm.tqdm(total=total_files)
        compile_rules()
        for target in args.file:
            scan_file(target)
            pbar.update(1)
        
        pbar.close()
        results = []
        for classifier in classifiers:
            print(f"Classifier: {classifier}")
            del classifiers[classifier]['instance']
            del classifiers[classifier]['save_path']
            del classifiers[classifier]['trained_data']
            classifiers[classifier]['total'] = total_files
            rate = None
            if args.malware:
                rate = classifiers[classifier]['results']/classifiers[classifier]['total']
            else:
                success_count = classifiers[classifier]['total'] - classifiers[classifier]['results']
                rate = success_count / classifiers[classifier]['total']
            
            classifiers[classifier]['rate'] = f"{ceil(rate*100)}%"
            print(f"{classifiers[classifier]['results']}/{total_files}",end=" = ")
            print(f"Precision: {classifiers[classifier]['rate']}")
        
        with open(save_file, 'w') as result_fd:
            result_fd.write(json.dumps(classifiers))
