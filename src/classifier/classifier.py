import os, sys
import argparse
import json
import pickle
import numpy as np

from sklearn.feature_extraction import FeatureHasher
from sklearn.ensemble import RandomForestClassifier


from feature_extractor import get_features

classifiers = {
    'Random Forest': {
        'instance': RandomForestClassifier(64),
        'save_path': 'random_forest_database.pkl'
    }
}


def process_features(args):
    features, hasher, pbar = args
    sample_type = features.pop('malicious')

    hashed_features = hasher.transform([features])
    hashed_features = hashed_features.toarray()[0]

    return hashed_features, sample_type


def train(dataset):
    hasher = FeatureHasher(2000)

    X = []
    y = []
    print(f"Processing {len(dataset)} features...")
    for feature in dataset:
        args = (feature, hasher, None)
        hashed, sample_type = process_features(args)
        X.append(hashed)
        y.append(sample_type)
        print(y)


    for classifier_name, classifier_data in classifiers.items():
        classifier = classifier_data.get('instance')
        database_path = classifier_data.get('save_path')

        print(f"Training with {classifier_name}")
        classifier.fit(X,y)

        print(f"Saving at {database_path}...")
    
        with open(database_path, "wb") as temp_db:
            pickle.dump((classifier, hasher), temp_db)

        print("Saved")


def scan_file(file_path):
    features = get_features(file_path)

    if features is None:
        return None

    features = dict(features)
    for classifier_name,  classifier_data in classifiers.items():
        print(f"Starting scan using {classifier_name}")
        database_path = classifier_data.get('save_path')
        
        if not os.access(database_path, os.F_OK):
            print(f"Unable to open {database_path}")
            continue

        with open(database_path, "rb") as saved_detector:
            classifier, hasher = pickle.load(saved_detector)

        hashed_features, s_type = process_features((features, hasher, None))
        results_prob = classifier.predict_proba([hashed_features])[:,-1]
        #print(hashed_features)
        if results_prob > 0.5:
            print(f"{file_path} is bad")
        else:
            print(f"{file_path} is good")




def load_dataset(dataset_path):
    if not os.access(dataset_path, os.F_OK):
        raise Exception("Unable to access {}".format(dataset_path))

    dataset = []
    with open(dataset_path, "r") as dataset_fd:
        for line in dataset_fd:
            dict_loaded = json.loads(line)
            dataset.append(dict_loaded)
    
    return dataset




if __name__ == '__main__':
    parser = argparse.ArgumentParser("Scan PE files using a classifier")
    parser.add_argument("--file", help="Path to file to do the scan")
    parser.add_argument("--train", help="Train our models first", action='store_true')
    parser.add_argument("--dataset", help="dataset path if we are training", type=str, default='')
    args = parser.parse_args()

    if args.train:
        if args.dataset == '':
            print("You should pass the dataset path")
            sys.exit(1)
        
        dataset = load_dataset(args.dataset)
        train(dataset)

    
    if args.file != '':
        scan_file(args.file)
        
