#!/usr/bin/python3

import pefile
import hashlib
import ssdeep, tlsh
import yara
import argparse
import sys
import os
import json
import glob
import tqdm
import warnings
from multiprocessing.dummy import Pool as ThreadPool    

from collections import OrderedDict 

YARA_RULES_PATH = "rules/"

class Extractor:
    def __init__(self, pe_path = None):
        self.pe_path = pe_path
        self.pe = None
        self.pe_data = b''
        self.yara_rules = []
        self.ENTROPY_TRESHOLD = 7.1
        self.known_exec_sec = {'.text':1, 'CODE': 1, '.data' :1, '.rsrc':1,  '.tls':1, '.idata': 1, '.bss' : 1, '.CRT': 1}
        self.no_useful_dlls = {'msvcrt.dll'}
        if pe_path != None:
            self.__load_pe()
    
    def load_pe(self, pe_path):
        self.pe_path = pe_path
        return self.__load_pe()
    

    def __load_pe(self):
        try:
            with open(self.pe_path, 'rb') as pe_fd:
                self.pe_data = pe_fd.read()

            self.pe = pefile.PE(data=self.pe_data)
            return True
        except Exception as e :
            return False

    def get_sections(self):
        return [s.Name.rstrip(b'\x00').decode(errors = 'ignore') for s in self.pe.sections]
    
    def get_imports(self):
        imports = []

        if not hasattr(self.pe, 'DIRECTORY_ENTRY_IMPORT'):
            return imports

        for entry in self.pe.DIRECTORY_ENTRY_IMPORT:
            dll_name = entry.dll.decode().lower()
            if dll_name in self.no_useful_dlls: continue
            for imp in entry.imports:
                name = imp.name
                # If is a ordinal import
                if not imp.name:
                    funcname = pefile.ordlookup.ordLookup(dll_name, imp.ordinal, make_name=True)
                    if funcname:
                        name = funcname

                imports.append((dll_name, name.decode()))

        return imports

    def get_sections_hash(self):
        sections = self.pe.sections
        section_hashs = []
        for section in sections:
            sec_hash = hashlib.sha256(section.get_data()).digest().hex()
            sec_name = section.Name.rstrip(b'\x00').decode(errors = 'ignore')
            sec_hash = (sec_name, sec_hash)
            section_hashs.append(sec_hash)
        
        return section_hashs

    def get_executable_sections(self):
        sections = self.pe.sections
        exec_sections = []
        for section in sections:
            flag = section.Characteristics & ~0xffffff
            sec_name = section.Name.rstrip(b'\x00').decode(errors = 'ignore')
            # 0x6 - read and execute (r-x)
            # 0xe - read write and execute (rwx)
            if sec_name not in self.known_exec_sec and (flag == 0x60000000 or flag == 0xe0000000):
                exec_sections.append(sec_name)

        return exec_sections

    def get_suspicious_sections(self):
        sections = self.pe.sections
        suspicious_sections = []
        for section in sections:
            flag = section.Characteristics & ~0xffffff
            sec_name = section.Name.rstrip(b'\x00').decode(errors = 'ignore')
            # 0xe - read write and execute (rwx)
            # 0xc - read and write

            if (flag == 0xe0000000 or flag == 0xc0000000) and sec_name not in self.known_exec_sec:
                suspicious_sections.append(sec_name)
            # Have a normal read and exec section but not have a known name
            elif flag == 0x60000000 and sec_name not in self.known_exec_sec:
                suspicious_sections.append(sec_name)
            # If the virtual size is greater than the size in disk, maybe this will change in runtime
            elif section.Misc_VirtualSize > section.SizeOfRawData:
                suspicious_sections.append(sec_name)
        
        return suspicious_sections

    def check_packed(self, use_yara=False, rules = []):
        if use_yara:
            for rule in rules:
                matchs = rule.match(data=self.pe_data)
                if matchs:
                    return True
    
        for section in self.pe.sections:
            if section.get_entropy() >= self.ENTROPY_TRESHOLD:
                return True
                
        return False


def load_and_compile_yara(yara_rules_path):
    yara_rules = []
    for root, dirs, files in os.walk(yara_rules_path):
        for f in files:
            full_path = os.path.join(root, f)
            with open(full_path, "r") as fd:
                try:
                    rule = yara.compile(file=fd)
                    yara_rules.append(rule)
                except:
                    continue
    return yara_rules


# dump each feature in each line
def dump_dataset(features):
    feature_str = ''
    for feature in features:
        feat_str = json.dumps(feature)
        feature_str += feat_str + '\n'

    return feature_str

def compile_rules():
    global yara_rules
    yara_rules = load_and_compile_yara(YARA_RULES_PATH)

def get_features(pe_path):
    global features
    features = []
    # wrapper
    class Dummy:
        def update(self, i):
            pass
    
    d = Dummy()
    args = (pe_path,d,-1, False)
    process_pe(args)
    
    if len(features) > 0:
        return features[0]  
    else:
        return None

def process_pe(arg):
    pe_path, bar, benign, do_analytics = arg
    feature_extractor = Extractor()
    if feature_extractor.load_pe(pe_path):
        feature_dict = OrderedDict()

        feature_dict['is_packed'] = 1 if feature_extractor.check_packed(use_yara=True, rules=yara_rules) else 0
        feature_dict['malicious'] = benign

        if do_analytics:
            analytics['packed'] += 1 if feature_dict['is_packed'] == 1 else 0
            analytics['malicious'] += 1 if feature_dict['malicious'] else 0

        exec_sections = feature_extractor.get_executable_sections()
        suspicious_sections = feature_extractor.get_suspicious_sections()
        sections = feature_extractor.get_sections()
        sections_hash = feature_extractor.get_sections_hash()
        imports = feature_extractor.get_imports()

        for section in sections:
            feature_dict['section/{}'.format(section)] = 1
            if do_analytics:
                analytics['sections'].setdefault(section, 0)
                analytics['sections'][section] += 1
        
        for exec_section in exec_sections:
            feature_dict['exec_section/{}'.format(exec_section)] = 1

            if do_analytics:
                analytics['exec_sections'].setdefault(exec_section, 0)
                analytics['sections'][exec_section] += 1
        
        for suspicious_section in suspicious_sections:
            feature_dict['suspicious_section/{}'.format(suspicious_section)] = 1

            if do_analytics:
                analytics['suspicious_section'].setdefault(suspicious_section, 0)
                analytics['suspicious_section'][suspicious_section] += 1

        
        for section_hash in sections_hash:
            feature_dict['hash/section/{}/{}'.format(section_hash[0], section_hash[1])] = 1
            if do_analytics:
                section_hash = f'{section_hash[0]}:{section_hash[1]}'
                analytics['sec_hash'].setdefault(section_hash, 0)
                analytics['sec_hash'][section_hash] += 1
        
        for iat in imports:
            import_name = f'{iat[0]}:{iat[1]}'
            feature_dict['import/{}/{}'.format(iat[0], iat[1])] = 1
            if do_analytics:
                analytics['imports'].setdefault(import_name, 0)
                analytics['imports'][import_name] += 1
        

        features.append(feature_dict)
    
    bar.update(1)


if __name__ == '__main__':
    args = argparse.ArgumentParser(description="PE feature extractor")
    args.add_argument("pe_folder", help="Path to the folder where is the PE files", type=str)
    args.add_argument("--output", "-o", help="Output where the dataset should be saved", type=str, default='dataset.txt')
    args.add_argument("--format", "-f", help="Format that the dataset should be exported, default = csv", default="csv", type=str)
    args.add_argument("--benign", help="Flag that indicates that this dataset is from benign software", action='store_true', default=False)
    args = args.parse_args()

    pe_path = args.pe_folder
    
    args.benign = 0 if args.benign else 1

    
    global yara_rules
    global features
    global analytics
    features = []
    analytics = {}
    analytics['packed'] = 0
    analytics['malicious'] = 0
    analytics['sections'] = {}
    analytics['suspicious_section'] = {}
    analytics['exec_sections'] = {}
    analytics['sec_hash'] = {}
    analytics['imports'] = {}

    yara_rules = load_and_compile_yara(YARA_RULES_PATH)


    print(f"Starting extractor for folder {pe_path}...")

    p = ThreadPool(os.cpu_count())
    pbar = tqdm.tqdm()
    th_args = [(f, pbar, args.benign, True) for f in glob.glob(f'{pe_path}*')]
    pbar.total = len(th_args)

    p.map(process_pe, th_args)

    p.close()
    p.join()

    if len(features) > 0:
        csv = dump_dataset(features)
        analytics_str = json.dumps(analytics)
        save_name = f'{args.output}.json'
        with open(args.output, 'w') as out_fd:
            out_fd.write(csv)
            print("[+] Saved dataset as {} [+]".format(args.output))
        
        with open(save_name, 'w') as analytics_fd:
            analytics_fd.write(analytics_str)
            print(f"[+] Analytics dataset savaed as {save_name} [+]")
        

    

                


        
