#!/usr/bin/python3

import pefile
import hashlib
import ssdeep
import yara
import argparse
import sys
import os

from collections import OrderedDict 

YARA_RULES_PATH = "rules/"

class Extractor:
    def __init__(self, pe_path = None, yara_rules_path = None):
        self.pe_path = pe_path
        self.pe = None
        self.pe_data = b''
        self.yara_rules_path = yara_rules_path
        self.yara_rules = []
        if pe_path != None:
            self.__load_pe()
        
        if yara_rules_path != None:
            self.__load_and_compile_yara()

    
    def load_pe(self, pe_path):
        self.pe_path = pe_path
        self.__load_pe()
    
    def __load_pe(self):
        try:
            with open(self.pe_path, 'rb') as pe_fd:
                self.pe_data = pe_fd.read()

            self.pe =pefile.PE(data=self.pe_data)
        except Exception as e :
            print("Error on parse PE {}".format(pe_path))


    def __load_and_compile_yara(self):
        for root, dirs, files in os.walk(self.yara_rules_path):
            for f in files:
                full_path = os.path.join(root, f)
                with open(full_path, "r") as fd:
                    try:
                        rule = yara.compile(file=fd)
                        self.yara_rules.append(rule)
                    except:
                        continue
            
    def get_imphash(self):
        return self.pe.get_imphash()
    
    def get_impfuzzy(self):
        '''
        Extract ImpFuzzy by taking all the imports strings separeted by a commoan and hashing with a fuzzy hash, ssdeep or tlsh

        '''
        if not hasattr(self.pe, 'DIRECTORY_ENTRY_IMPORT'):
            return None

        imports = ''
        for entry in self.pe.DIRECTORY_ENTRY_IMPORT:
            dll_name = entry.dll.decode().lower().split('.')[0] # remove .dll
            
            for imp in entry.imports:
                import_name = imp.name
                if import_name is None:
                    import_name = pefile.ordlookup.ordLookup(entry.dll.lower(), imp.ordinal, make_name=True)
                    if import_name is None:
                        return None

                    
                import_name = import_name.decode().lower()
                imports += '{}.{},'.format(dll_name, import_name)

        imports = imports[:-1] # remove the last ','


        return ssdeep.hash(imports)
    

    def get_ssdeep(self):
        return ssdeep.hash(self.pe_data)

    def check_packed(self, use_yara):
        if use_yara:
            for rule in self.yara_rules:
                matchs = rule.match(data=self.pe_data)
                if matchs:
                    return True
    
        # for section in self.pe.get_sections():
        #     if peutils.calc_entropy(section.get_data()) >= 7.9:
        #         return True

        # # Native library
        # sections_entropy = peutils.calc_entropy()

        # for entropy in sections:
        #     if entropy >= 7.7:
        #         return True
            
        return False


def csv_from_dict_list(d):
    if len(d) == 0:
        return None
    
    csv_keys = list(d[0].keys())
    csv_header = ','.join(csv_keys)
    csv_content = ''
    for dictionary in d:
        for k, v in dictionary.items():
            csv_content += '{},'.format(v)

        csv_content = csv_content[:-1] + '\n'
    

    csv_content = '{}\n{}'.format(csv_header, csv_content)

    return csv_content
            


if __name__ == '__main__':
    args = argparse.ArgumentParser(description="PE feature extractor")
    args.add_argument("pe_folder", help="Path to the folder where is the PE files", type=str)
    args.add_argument("--output", "-o", help="Output where the dataset should be saved", type=str, default='dataset.txt')
    args.add_argument("--format", "-f", help="Format that the dataset should be exported, default = csv", default="csv", type=str)
    args = args.parse_args()

    pe_path = args.pe_folder
    
    feature_extractor = Extractor(yara_rules_path=YARA_RULES_PATH)
    features = []

    for root, dirs, files in os.walk(pe_path):
        for f in files:
            full_path = os.path.join(root, f)
            is_pe = False 
            with open(full_path, "rb") as fd:
                is_pe = fd.read(2) == b'MZ'
            
            if is_pe:
                # print("Sample: {}".format(f))
                feature_extractor.load_pe(full_path)
                feature_dict = OrderedDict()
                feature_dict['imphash'] = feature_extractor.get_imphash()
                feature_dict['impfuzzy'] = feature_extractor.get_impfuzzy()
                feature_dict['is_packed'] = feature_extractor.check_packed(use_yara=True)
                feature_dict['ssdeep'] = feature_extractor.get_ssdeep()
                #feature_dict['strings_fuzzy'] = feature_extractor.fuzzy_strings(pattern=feature_extractor.C2_PATTERNS)

                #     'imphash': feature_extractor.get_imphash(),
                #     'impfuzzy': feature_extractor.get_impfuzzy(),
                #     'is_packed': feature_extractor.check_packed(use_yara=True),
                #     # 'suspicious_timestamp': feature_extractor.check_timestamp(),
                #     # 'has_padding': feature_extractor.has_padding(),
                #     # 'strings':  feature_extractor.extract_strings(pattern=feature_extractor.C2_PATTERNS)
                #      'ssdeep':   feature_extractor.get_ssdeep(),
                #     # 'tlsh': feature_extractor.has_padding(),
                #     # 'sha256': feature_extractor.get_sha256()
                # }
                
                features.append(feature_dict)

#                for feature_name, value in features_dict.items():
#                    print("{} - {}".format(feature_name, value))
    
    if len(features) > 0:
        if args.format == 'csv':
            csv = csv_from_dict_list(features)
            with open(args.output, 'w') as out_fd:
                out_fd.write(csv)
                print("[+] Saved as {} [+]".format(args.output))

    

                


        
